#+startup hidestars indent showall
#+title: note on Spark SQL source code

* Why Spark SQL
  :PROPERTIES:
  :ID:       6bcafa0c-1109-4aa7-80f1-2140c8e98570
  :END:

* Catalyst optimizer
  :PROPERTIES:
  :ID:       17e0553b-2afd-4019-992a-d41408fcdbad
  :END:
** Analysis ([[file:~/Dropbox/git/spark/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala][Analyzer)]]
   :PROPERTIES:
   :ID:       2d56b2dc-6a21-4cfb-89e4-7f7b285dc8c5
   :END:
  - Provides a logical query plan analyzer, which translates UnresolvedAttributes and UnresolvedRelations into fully typed objects using information in a SessionCatalog.
  - Provides a way to keep state during the analysis, this enables us to decouple the concerns

*** An attribute is called unresolved if we do not know its type or have not matched it to an input table ( or an alias)
    :PROPERTIES:
    :ID:       a41ebfaa-d94b-4d31-94a2-402442a854a9
    :END:
*** Spark SQL uses Catalyst rules and Catalog object that tracks the tables in all data sources to resolve these attributes.
    :PROPERTIES:
    :ID:       7fa7573d-88e6-4a0f-a71f-7295659541f0
    :END:

** Logical optimization ([[file:~/Dropbox/git/spark/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala::/*][Optimizer)]]
   :PROPERTIES:
   :ID:       6f2bfc71-4dc1-484c-8e21-4fbbd39465b4
   :END:
*** [[file:~/Dropbox/git/spark/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala::val%20operatorOptimizationRuleSet%20=][three kinds of operatorOptimizationRule]]
    :PROPERTIES:
    :ID:       41c198df-d1a8-406c-b9e6-190f374adf63
    :END:
    - Push down


    - Combine
      - CombineFilters

    - Constant folding and strength reduction

*** [[file:~/Dropbox/git/spark/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala::object%20CombineFilters%20extends%20Rule%5BLogicalPlan%5D%20with%20PredicateHelper%20{][CombineFilters]]
    :PROPERTIES:
    :ID:       beba84da-d9a3-41be-8ed1-595d3508a30b
    :END:
    - [[file:~/Dropbox/git/spark/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/trees/TreeNode.scala::def%20transform(rule:%20PartialFunction%5BBaseType,%20BaseType%5D):%20BaseType%20=%20{][transform]] (transformDown and transformUp)
      - Returns a copy of this node where `rule` has been recursively applied to the tree.
      - tranformDown or tranformUp is directionality is expected.
    - [[file:~/Dropbox/git/spark/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/trees/TreeNode.scala::object%20CurrentOrigin%20{][CurrentOrigin]]
      - Provides a location for TreeNodes to ask about the context of their origin.  For example, which line of code is currently being parsed.

    - [[file:~/.m2/repository/org/scala-lang/scala-library/2.11.8/scala-library-2.11.8-sources.jar:scala/PartialFunction.scala::/*%20__%20*\][PartialFunction]]
     - PF[A, B] where PF is defined at a subset of A (eg. divsiion)
     - [[http://sandrasi-sw.blogspot.com/2012/03/understanding-scalas-partially-applied.html][partial function vs partially applied function]]



    - [[file:~/Dropbox/git/spark/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/trees/TreeNode.scala::def%20mapChildren(f:%20BaseType%20=>%20BaseType):%20BaseType%20=%20{][mapChildren]]
      - Returns a copy of this node where `f` has been applied to all the nodes children.
       - return original TreeNode if no change happens
       - m: Map => m.mapValues
       - d: DataType => d  // Avoid unpacking structs
       -

*** [[file:~/Dropbox/git/spark/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala::ReorderJoin,][ReorderJoin]]
    :PROPERTIES:
    :ID:       203910f8-49e1-4c9f-915d-f421a2181948
    :END:
    - [[https://stackoverflow.com/questions/35130247/how-to-inject-traits-to-base-type-classes-to-use-them-in-generic-type-methods][joins.scala]]
      - [[https://www.scala-lang.org/api/2.12.3/scala/annotation/tailrec.html][@tailrec]]
        - A method annotation which verifies that the method will be compiled with tail call optimization. If it is present, the compiler will issue an error if the method cannot be optimized into a loop.
    - [[file:~/.m2/repository/org/scala-lang/scala-library/2.11.8/scala-library-2.11.8-sources.jar:scala/collection/TraversableLike.scala::def%20partition(p:%20A%20=>%20Boolean):%20(Repr,%20Repr)%20=%20{][partition]]


*** [[file:~/Dropbox/git/spark/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala::val%20operatorOptimizationBatch:%20Seq%5BBatch%5D%20=%20{][3-stage batch]]
    :PROPERTIES:
    :ID:       6297cca1-b62d-4493-8a0f-4494b16b8483
    :END:

   - Batch is a set of rules

*** Remaining questions
    :PROPERTIES:
    :ID:       71808752-383a-4203-8d7f-d8b3ee5f836f
    :END:
**** [[file:~/Dropbox/git/spark/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala::object%20SimpleTestOptimizer%20extends%20SimpleTestOptimizer][Why define SimpleTestOptimizer object?]]
     :PROPERTIES:
     :ID:       7f56448e-ed2f-4388-83d3-fa83edba53c8
     :END:
    - Define this optimizer to use in the test code?

**** Cost-based vs rule-based optimization
     :PROPERTIES:
     :ID:       d516cd32-4d1b-4d81-8826-f943fbcf24f5
     :END:
****
     :PROPERTIES:
     :ID:       f460162f-36da-4416-a39b-b21143127b27
     :END:


** Logical plan to physical plan ([[file:~/Dropbox/git/spark/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala::/*][SparkStrategies.scala]])
   :PROPERTIES:
   :ID:       16477982-85ae-43ef-8e5c-a663c5f9b1b8
   :END:
   - Logical plan --> SparkPlan --> Query Plan --> TreeNode --> Product --> scala.Any

** Code generation
   :PROPERTIES:
   :ID:       af11d0dc-ca9a-4b61-8687-55926179748e
   :END:
*** [[https://docs.scala-lang.org/overviews/quasiquotes/intro.html][Quasiquote]]
    :PROPERTIES:
    :ID:       05118ed2-e718-4ee4-8945-d566074bd671
    :END:



** Limits of Catalyst optimizer
   :PROPERTIES:
   :ID:       0850e97b-59d0-402e-affb-6ddbd3fb201c
   :END:
*** Cannot optimize functional operation (such as filer)
    :PROPERTIES:
    :ID:       0ac45a9e-a523-4c70-9d34-14163f91dd41
    :END:
    - ds.filter(p => p.city == "Boston")
    - it is impossible for Catalyst to introspect the lambda function
    - introspect scala function definition is not possible in Scala


* References
  :PROPERTIES:
  :ID:       a9cf4e02-2ec1-4cf6-8413-39d413c29e5f
  :END:
** [[https://databricks.com/blog/2015/04/13/deep-dive-into-spark-sqls-catalyst-optimizer.html][Databricks blog on Catalyst]]
   :PROPERTIES:
   :ID:       6e97a5a4-a6c9-4569-b984-37d09028d331
   :END:
